{"cells":[{"cell_type":"markdown","metadata":{"id":"Av6GwRuLMfJX"},"source":["# PatchTSMixer in HuggingFace - Getting Started\n"]},{"cell_type":"markdown","metadata":{"id":"yGgL1_vvMfJY"},"source":["`PatchTSMixer` is a lightweight time-series modeling approach based on the MLP-Mixer architecture. It is proposed in [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by IBM Research authors `Vijay Ekambaram`, `Arindam Jati`, `Nam Nguyen`, `Phanwadee Sinthong` and `Jayant Kalagnanam`.\n","\n","For effective mindshare and to promote opensourcing - IBM Research join hands with the HuggingFace team to opensource this model in HF.\n","\n","In this [HuggingFace implementation](https://huggingface.co/docs/transformers/main/en/model_doc/patchtsmixer), we provide PatchTSMixer’s capabilities to effortlessly facilitate lightweight mixing across patches, channels, and hidden features for effective multivariate time-series modeling. It also supports various attention mechanisms starting from simple gated attention to more complex self-attention blocks that can be customized accordingly. The model can be pretrained and subsequently used for various downstream tasks such as forecasting, classification, and regression.\n","\n","`PatchTSMixer` outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X). For more details, refer to the [paper](https://arxiv.org/pdf/2306.09364.pdf)\n","\n","In this blog, we will demonstrate examples of getting started with PatchTSMixer. We will first demonstrate the forecasting capability of `PatchTSMixer` on the Electricity data. We will then demonstrate the transfer learning capability of PatchTSMixer by using the model trained on the Electricity to do zero-shot forecasting on the ETTH2 dataset.\n","\n","\n","`Blog authors`: Arindam Jati, Vijay Ekambaram, Nam Ngugen, Wesley Gifford and Kashif Rasul\n"]},{"cell_type":"markdown","metadata":{"id":"0-CgpiCCMfJZ"},"source":["## Installation\n","This demo needs Huggingface [`transformers`](https://github.com/huggingface/transformers) for main modeling tasks, and IBM `tsfm` for auxiliary data pre-processing.\n","We can install both by cloning the `tsfm` repository and following the below steps.\n","\n","1. Clone IBM Time Series Foundation Model Repository [`tsfm`](https://github.com/ibm/tsfm).\n","    ```\n","    git clone git@github.com:IBM/tsfm.git\n","    cd tsfm\n","    ```\n","2. Install `tsfm`. This will also install Huggingface `transformers`.\n","    ```\n","    pip install .\n","    ```\n","3. Test it with the following commands in a `python` terminal.\n","    ```\n","    from transformers import PatchTSMixerConfig\n","    from tsfm_public.toolkit.dataset import ForecastDFDataset\n","    ```"]},{"cell_type":"markdown","metadata":{"id":"TFMXBwh0MfJZ"},"source":["## Part 1: Forecasting on Electricity dataset"]},{"cell_type":"code","source":["# Clone the IBM tsfm repository\n","!git clone https://github.com/IBM/tsfm.git\n","%cd tsfm\n","\n","# Install the package and its dependencies\n","!pip install ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"QfqjeY8eOCn5","executionInfo":{"status":"ok","timestamp":1742402471447,"user_tz":-480,"elapsed":129631,"user":{"displayName":"Zeyang Liu","userId":"01434823736905118983"}},"outputId":"ab160fe6-7638-4108-88d9-128e71c03974"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'tsfm'...\n","remote: Enumerating objects: 8459, done.\u001b[K\n","remote: Counting objects: 100% (2487/2487), done.\u001b[K\n","remote: Compressing objects: 100% (606/606), done.\u001b[K\n","remote: Total 8459 (delta 2032), reused 1903 (delta 1881), pack-reused 5972 (from 2)\u001b[K\n","Receiving objects: 100% (8459/8459), 48.90 MiB | 15.04 MiB/s, done.\n","Resolving deltas: 100% (5633/5633), done.\n","/content/tsfm\n","Processing /content/tsfm\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from granite-tsfm==2rc3.dev14+gf6cfbf1) (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from granite-tsfm==2rc3.dev14+gf6cfbf1) (1.6.1)\n","Requirement already satisfied: transformers>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (4.48.3)\n","Collecting datasets (from granite-tsfm==2rc3.dev14+gf6cfbf1)\n","  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: deprecated in /usr/local/lib/python3.11/dist-packages (from granite-tsfm==2rc3.dev14+gf6cfbf1) (1.2.18)\n","Requirement already satisfied: urllib3<3,>=1.26.19 in /usr/local/lib/python3.11/dist-packages (from granite-tsfm==2rc3.dev14+gf6cfbf1) (2.3.0)\n","Collecting numpy<2 (from granite-tsfm==2rc3.dev14+gf6cfbf1)\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (2025.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (0.28.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (4.67.1)\n","Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (2.6.0+cu124)\n","Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (1.3.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->granite-tsfm==2rc3.dev14+gf6cfbf1) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets->granite-tsfm==2rc3.dev14+gf6cfbf1)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Collecting xxhash (from datasets->granite-tsfm==2rc3.dev14+gf6cfbf1)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets->granite-tsfm==2rc3.dev14+gf6cfbf1)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->granite-tsfm==2rc3.dev14+gf6cfbf1) (2024.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->granite-tsfm==2rc3.dev14+gf6cfbf1) (3.11.13)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated->granite-tsfm==2rc3.dev14+gf6cfbf1) (1.17.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->granite-tsfm==2rc3.dev14+gf6cfbf1) (1.14.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->granite-tsfm==2rc3.dev14+gf6cfbf1) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->granite-tsfm==2rc3.dev14+gf6cfbf1) (3.6.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (5.9.5)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->granite-tsfm==2rc3.dev14+gf6cfbf1) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->granite-tsfm==2rc3.dev14+gf6cfbf1) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->granite-tsfm==2rc3.dev14+gf6cfbf1) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->granite-tsfm==2rc3.dev14+gf6cfbf1) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->granite-tsfm==2rc3.dev14+gf6cfbf1) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->granite-tsfm==2rc3.dev14+gf6cfbf1) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->granite-tsfm==2rc3.dev14+gf6cfbf1) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers>=4.38.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (4.12.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (2025.1.31)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->transformers[torch]>=4.38.0->granite-tsfm==2rc3.dev14+gf6cfbf1) (3.0.2)\n","Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.4.1-py3-none-any.whl (487 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: granite-tsfm\n","  Building wheel for granite-tsfm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for granite-tsfm: filename=granite_tsfm-2rc3.dev14+gf6cfbf1-py3-none-any.whl size=2341614 sha256=c0d1420eb34f281f51b688ae5b26b1c6549ae839cf29bac8224ee537807a63e9\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-pla4h31j/wheels/b0/e9/d5/46ac6e5f659a03cfc8e396b992ac1b7e0c0bd1e34c7f5fe8e4\n","Successfully built granite-tsfm\n","Installing collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, granite-tsfm\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed datasets-3.4.1 dill-0.3.8 granite-tsfm-2rc3.dev14+gf6cfbf1 multiprocess-0.70.16 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"]}]},{"cell_type":"code","source":["!pip uninstall numpy -y\n","!pip install numpy==1.24.3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":466},"collapsed":true,"id":"9eh6ExVTXdSt","executionInfo":{"status":"ok","timestamp":1742402500188,"user_tz":-480,"elapsed":17630,"user":{"displayName":"Zeyang Liu","userId":"01434823736905118983"}},"outputId":"1a0793b0-0a0f-4df6-b648-a11be09031b3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: numpy 1.26.4\n","Uninstalling numpy-1.26.4:\n","  Successfully uninstalled numpy-1.26.4\n","Collecting numpy==1.24.3\n","  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n","Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n","albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n","treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n","pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n","tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n","jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n","jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n","blosc2 3.2.0 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.24.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"d5fffb66fea44c5c8ab722146a97d439"}},"metadata":{}}]},{"cell_type":"code","source":["!pip install --upgrade numpy==1.24.3 jax tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"idAbVVseXeYR","executionInfo":{"status":"ok","timestamp":1742402598038,"user_tz":-480,"elapsed":88662,"user":{"displayName":"Zeyang Liu","userId":"01434823736905118983"}},"outputId":"10aff473-7792-4b4d-8e8d-2d56d02634c7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.11/dist-packages (1.24.3)\n","Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (0.5.2)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Collecting tensorflow\n","  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n","Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax) (0.5.1)\n","Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax) (0.4.1)\n","INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n","Collecting jax\n","  Downloading jax-0.5.2-py3-none-any.whl.metadata (22 kB)\n","  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n","  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.5.0,>=0.5.0 (from jax)\n","  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n","Collecting jax\n","  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.4.38,>=0.4.38 (from jax)\n","  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax) (3.4.0)\n","Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax) (1.14.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n","Collecting tensorboard~=2.19.0 (from tensorflow)\n","  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n","Collecting tensorflow\n","  Downloading tensorflow-2.18.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","  Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n","  Downloading tensorflow-2.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n","Collecting tensorboard<2.18,>=2.17 (from tensorflow)\n","  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","Downloading jax-0.4.38-py3-none-any.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow-2.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl (101.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboard, jaxlib, jax, tensorflow\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.18.0\n","    Uninstalling tensorboard-2.18.0:\n","      Successfully uninstalled tensorboard-2.18.0\n","  Attempting uninstall: jaxlib\n","    Found existing installation: jaxlib 0.5.1\n","    Uninstalling jaxlib-0.5.1:\n","      Successfully uninstalled jaxlib-0.5.1\n","  Attempting uninstall: jax\n","    Found existing installation: jax 0.5.2\n","    Uninstalling jax-0.5.2:\n","      Successfully uninstalled jax-0.5.2\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.18.0\n","    Uninstalling tensorflow-2.18.0:\n","      Successfully uninstalled tensorflow-2.18.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.17.1 which is incompatible.\n","tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.17.1 which is incompatible.\n","orbax-checkpoint 0.11.8 requires jax>=0.5.0, but you have jax 0.4.38 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed jax-0.4.38 jaxlib-0.4.38 tensorboard-2.17.1 tensorflow-2.17.1\n"]}]},{"cell_type":"code","source":["from transformers import PatchTSMixerConfig\n","from tsfm_public.toolkit.dataset import ForecastDFDataset\n"],"metadata":{"id":"bcL7__VvQMXD","executionInfo":{"status":"ok","timestamp":1742402647265,"user_tz":-480,"elapsed":27547,"user":{"displayName":"Zeyang Liu","userId":"01434823736905118983"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"S-mFmWzZMfJa","executionInfo":{"status":"ok","timestamp":1742402653737,"user_tz":-480,"elapsed":2052,"user":{"displayName":"Zeyang Liu","userId":"01434823736905118983"}}},"outputs":[],"source":["# Standard\n","import os\n","import random\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","# Third Party\n","from transformers import (\n","    EarlyStoppingCallback,\n","    PatchTSMixerConfig,\n","    PatchTSMixerForPrediction,\n","    Trainer,\n","    TrainingArguments,\n",")\n","\n","# First Party\n","from tsfm_public.toolkit.dataset import ForecastDFDataset\n","from tsfm_public.toolkit.time_series_preprocessor import TimeSeriesPreprocessor\n","from tsfm_public.toolkit.util import select_by_index"]},{"cell_type":"markdown","metadata":{"id":"PfijQDqIMfJa"},"source":[" ### Set seed"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ifYA59wiMfJb","executionInfo":{"status":"ok","timestamp":1742402656037,"user_tz":-480,"elapsed":3,"user":{"displayName":"Zeyang Liu","userId":"01434823736905118983"}}},"outputs":[],"source":["SEED = 42\n","torch.manual_seed(SEED)\n","random.seed(SEED)\n","np.random.seed(SEED)"]},{"cell_type":"markdown","metadata":{"id":"YATqxvK2MfJb"},"source":["### Load and prepare datasets\n","\n","In the next cell, please adjust the following parameters to suit your application:\n","- `PRETRAIN_AGAIN`: Set this to `True` if you want to perform pretraining again. Note that this might take some time depending on the GPU availability. Otherwise, the already pretrained model will be used.\n","- `dataset_path`: path to local .csv file, or web address to a csv file for the data of interest. Data is loaded with pandas, so anything supported by\n","`pd.read_csv` is supported: (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).\n","- `timestamp_column`: column name containing timestamp information, use None if there is no such column\n","- `id_columns`: List of column names specifying the IDs of different time series. If no ID column exists, use []\n","- `forecast_columns`: List of columns to be modeled\n","- `context_length`: The amount of historical data used as input to the model. Windows of the input time series data with length equal to\n","`context_length` will be extracted from the input dataframe. In the case of a multi-time series dataset, the context windows will be created\n","so that they are contained within a single time series (i.e., a single ID).\n","- `forecast_horizon`: Number of timestamps to forecast in future.\n","- `train_start_index`, `train_end_index`: the start and end indices in the loaded data which delineate the training data.\n","- `valid_start_index`, `valid_end_index`: the start and end indices in the loaded data which delineate the validation data.\n","- `test_start_index`, `test_end_index`: the start and end indices in the loaded data which delineate the test data.\n","- `patch_length`: The patch length for the `PatchTSMixer` model. It is recommended to choose a value that evenly divides `context_length`.\n","- `num_workers`: Number of dataloder workers in pytorch dataloader.\n","- `batch_size`: Batch size.\n","The data is first loaded into a Pandas dataframe and split into training, validation, and test parts. Then the pandas dataframes are converted\n","to the appropriate torch dataset needed for training."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Lr3kTZraMfJb","executionInfo":{"status":"ok","timestamp":1742403220168,"user_tz":-480,"elapsed":44,"user":{"displayName":"Zeyang Liu","userId":"01434823736905118983"}}},"outputs":[],"source":["PRETRAIN_AGAIN = True\n","# Download ECL data from https://github.com/zhouhaoyi/Informer2020\n","dataset_path = \"ECL.csv\"\n","timestamp_column = \"date\"\n","id_columns = []\n","\n","context_length = 512\n","forecast_horizon = 96\n","patch_length = 8\n","num_workers = 16  # Reduce this if you have low number of CPU cores\n","batch_size = 64  # Adjust according to GPU memory"]},{"cell_type":"code","source":["# import numpy as np\n","# # Parameters\n","# beta = 0.2\n","# gamma = 0.1\n","# n = 10\n","# tau = 17\n","# t_max = 11000\n","# dt = 1  # time step\n","# steps = int(t_max / dt)\n","\n","# # Delay in steps\n","# tau_steps = int(tau / dt)\n","\n","# # Initialize P with initial values (e.g., 1.2 for all)\n","# P = np.zeros(steps)\n","# P[:tau_steps+1] = 1.2  # history for the delay\n","\n","# # Time vector\n","# t = np.arange(0, t_max, dt)\n","\n","# # Numerical integration using Euler's method\n","# for i in range(tau_steps, steps - 1):\n","#     delayed = P[i - tau_steps]\n","#     dP = beta * delayed / (1 + delayed**n) - gamma * P[i]\n","#     P[i + 1] = P[i] + dP * dt\n","\n","# MGdata = pd.DataFrame({\n","#     'time': t,\n","#     'P': P\n","# })"],"metadata":{"id":"qJUVCbiFT1q_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"VEO5mNoVMfJc","executionInfo":{"status":"ok","timestamp":1742403225562,"user_tz":-480,"elapsed":4076,"user":{"displayName":"Zeyang Liu","userId":"01434823736905118983"}}},"outputs":[],"source":["if PRETRAIN_AGAIN:\n","    data = pd.read_csv(\n","        dataset_path,\n","        parse_dates=[timestamp_column],\n","    )\n","    forecast_columns = list(data.columns[1:])\n","\n","    # get split\n","    num_train = int(len(data) * 0.7)\n","    num_test = int(len(data) * 0.2)\n","    num_valid = len(data) - num_train - num_test\n","    border1s = [\n","        0,\n","        num_train - context_length,\n","        len(data) - num_test - context_length,\n","    ]\n","    border2s = [num_train, num_train + num_valid, len(data)]\n","\n","    train_start_index = border1s[0]  # None indicates beginning of dataset\n","    train_end_index = border2s[0]\n","\n","    # we shift the start of the evaluation period back by context length so that\n","    # the first evaluation timestamp is immediately following the training data\n","    valid_start_index = border1s[1]\n","    valid_end_index = border2s[1]\n","\n","    test_start_index = border1s[2]\n","    test_end_index = border2s[2]\n","\n","    train_data = select_by_index(\n","        data,\n","        id_columns=id_columns,\n","        start_index=train_start_index,\n","        end_index=train_end_index,\n","    )\n","    valid_data = select_by_index(\n","        data,\n","        id_columns=id_columns,\n","        start_index=valid_start_index,\n","        end_index=valid_end_index,\n","    )\n","    test_data = select_by_index(\n","        data,\n","        id_columns=id_columns,\n","        start_index=test_start_index,\n","        end_index=test_end_index,\n","    )\n","\n","    tsp = TimeSeriesPreprocessor(\n","        timestamp_column=timestamp_column,\n","        id_columns=id_columns,\n","        input_columns=forecast_columns,\n","        output_columns=forecast_columns,\n","        scaling=True,\n","    )\n","    tsp.train(train_data)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":239},"id":"fvkhNX0YMfJc","executionInfo":{"status":"error","timestamp":1742403231500,"user_tz":-480,"elapsed":1064,"user":{"displayName":"Zeyang Liu","userId":"01434823736905118983"}},"outputId":"3cf76772-ed63-4e01-88c3-da7831748913"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ForecastDFDataset.__init__() got an unexpected keyword argument 'input_columns'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-5c7fb544821e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mPRETRAIN_AGAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     train_dataset = ForecastDFDataset(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mtsp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mid_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtimestamp_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: ForecastDFDataset.__init__() got an unexpected keyword argument 'input_columns'"]}],"source":["if PRETRAIN_AGAIN:\n","    train_dataset = ForecastDFDataset(\n","        tsp.preprocess(train_data),\n","        id_columns=id_columns,\n","        timestamp_column=\"date\",\n","        input_columns=forecast_columns,\n","        output_columns=forecast_columns,\n","        context_length=context_length,\n","        prediction_length=forecast_horizon,\n","    )\n","    valid_dataset = ForecastDFDataset(\n","        tsp.preprocess(valid_data),\n","        id_columns=id_columns,\n","        timestamp_column=\"date\",\n","        input_columns=forecast_columns,\n","        output_columns=forecast_columns,\n","        context_length=context_length,\n","        prediction_length=forecast_horizon,\n","    )\n","    test_dataset = ForecastDFDataset(\n","        tsp.preprocess(test_data),\n","        id_columns=id_columns,\n","        timestamp_column=\"date\",\n","        input_columns=forecast_columns,\n","        output_columns=forecast_columns,\n","        context_length=context_length,\n","        prediction_length=forecast_horizon,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"bS16-FwXMfJc"},"source":[" ## Configure the PatchTSMixer model\n","\n"," The settings below control the different components in the PatchTSMixer model.\n","  - `num_input_channels`: the number of input channels (or dimensions) in the time series data. This is\n","    automatically set to the number for forecast columns.\n","  - `context_length`: As described above, the amount of historical data used as input to the model.\n","  - `prediction_length`: This is same as the forecast horizon as decribed above.\n","  - `patch_length`: The length of the patches extracted from the context window (of length `context_length``).\n","  - `patch_stride`: The stride used when extracting patches from the context window.\n","  - `d_model`: Hidden feature dimension of the model.\n","  - `num_layers`: The number of model layers.\n","  - `dropout`: Dropout probability for all fully connected layers in the encoder.\n","  - `head_dropout`: Dropout probability used in the head of the model.\n","  - `mode`: PatchTSMixer operating mode. \"common_channel\"/\"mix_channel\". Common-channel works in channel-independent mode. For pretraining, use \"common_channel\".\n","  - `scaling`: Per-widow standard scaling. Recommended value: \"std\".\n","\n","For full details on the parameters - refer [here](https://huggingface.co/docs/transformers/main/en/model_doc/patchtsmixer)\n","\n","We recommend that you only adjust the values in the next cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"quohTyMZMfJd"},"outputs":[],"source":["if PRETRAIN_AGAIN:\n","    config = PatchTSMixerConfig(\n","        context_length=context_length,\n","        prediction_length=forecast_horizon,\n","        patch_length=patch_length,\n","        num_input_channels=len(forecast_columns),\n","        patch_stride=patch_length,\n","        d_model=16,\n","        num_layers=8,\n","        expansion_factor=2,\n","        dropout=0.2,\n","        head_dropout=0.2,\n","        mode=\"common_channel\",\n","        scaling=\"std\",\n","    )\n","    model = PatchTSMixerForPrediction(config)"]},{"cell_type":"markdown","metadata":{"id":"tKkNluYbMfJd"},"source":[" ## Train model\n","\n"," Trains the PatchTSMixer model based on the direct forecasting strategy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H4fAGvucMfJd","outputId":"1f787b91-5342-4f71-99a0-f9188ff82dcb"},"outputs":[{"name":"stderr","output_type":"stream","text":["/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2450' max='7000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2450/7000 21:35 < 40:08, 1.89 it/s, Epoch 35/100]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.247100</td>\n","      <td>0.141067</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.168600</td>\n","      <td>0.127757</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.156500</td>\n","      <td>0.122327</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.150300</td>\n","      <td>0.118918</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.146000</td>\n","      <td>0.116496</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.143100</td>\n","      <td>0.114968</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.140800</td>\n","      <td>0.113678</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.139200</td>\n","      <td>0.113057</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.137900</td>\n","      <td>0.112405</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.136900</td>\n","      <td>0.112225</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.136100</td>\n","      <td>0.112087</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.135400</td>\n","      <td>0.112330</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.134700</td>\n","      <td>0.111778</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.134100</td>\n","      <td>0.111702</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.133700</td>\n","      <td>0.110964</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.133100</td>\n","      <td>0.111164</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.132800</td>\n","      <td>0.111063</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.132400</td>\n","      <td>0.111088</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.132100</td>\n","      <td>0.110905</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.131800</td>\n","      <td>0.110844</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.131300</td>\n","      <td>0.110831</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.131100</td>\n","      <td>0.110278</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.130700</td>\n","      <td>0.110591</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.130600</td>\n","      <td>0.110319</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.130300</td>\n","      <td>0.109900</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.130000</td>\n","      <td>0.109982</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.129900</td>\n","      <td>0.109975</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.129600</td>\n","      <td>0.110128</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.129300</td>\n","      <td>0.109995</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.129100</td>\n","      <td>0.109868</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.129000</td>\n","      <td>0.109928</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.128700</td>\n","      <td>0.109823</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.128500</td>\n","      <td>0.109863</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.128400</td>\n","      <td>0.109794</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.128100</td>\n","      <td>0.109945</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]}],"source":["if PRETRAIN_AGAIN:\n","    training_args = TrainingArguments(\n","        output_dir=\"./checkpoint/patchtsmixer/electricity/pretrain/output/\",\n","        overwrite_output_dir=True,\n","        learning_rate=0.001,\n","        num_train_epochs=100,  # For a quick test of this notebook, set it to 1\n","        do_eval=True,\n","        evaluation_strategy=\"epoch\",\n","        per_device_train_batch_size=batch_size,\n","        per_device_eval_batch_size=batch_size,\n","        dataloader_num_workers=num_workers,\n","        report_to=\"tensorboard\",\n","        save_strategy=\"epoch\",\n","        logging_strategy=\"epoch\",\n","        save_total_limit=3,\n","        logging_dir=\"./checkpoint/patchtsmixer/electricity/pretrain/logs/\",  # Make sure to specify a logging directory\n","        load_best_model_at_end=True,  # Load the best model when training ends\n","        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n","        greater_is_better=False,  # For loss\n","        label_names=[\"future_values\"],\n","        # max_steps=20,\n","    )\n","\n","    # Create the early stopping callback\n","    early_stopping_callback = EarlyStoppingCallback(\n","        early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n","        early_stopping_threshold=0.0001,  # Minimum improvement required to consider as improvement\n","    )\n","\n","    # define trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=valid_dataset,\n","        callbacks=[early_stopping_callback],\n","    )\n","\n","    # pretrain\n","    trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"0WMo4-FLMfJd"},"source":[" ## Evaluate model on the test set.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Wl-nG-gMfJd","outputId":"c74be0ec-9df2-4c6c-b22d-a29c7d30508d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [21/21 00:03]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Test result:\n","{'eval_loss': 0.12884521484375, 'eval_runtime': 5.7532, 'eval_samples_per_second': 897.763, 'eval_steps_per_second': 3.65, 'epoch': 35.0}\n"]}],"source":["if PRETRAIN_AGAIN:\n","    results = trainer.evaluate(test_dataset)\n","    print(\"Test result:\")\n","    print(results)"]},{"cell_type":"markdown","metadata":{"id":"-k3paWRmMfJd"},"source":["We get MSE score of 0.128 which is the SOTA result on the Electricity data."]},{"cell_type":"markdown","metadata":{"id":"3OESqzJrMfJd"},"source":[" ## Save model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7CdqlrilMfJe"},"outputs":[],"source":["if PRETRAIN_AGAIN:\n","    save_dir = \"patchtsmixer/electricity/model/pretrain/\"\n","    os.makedirs(save_dir, exist_ok=True)\n","    trainer.save_model(save_dir)"]},{"cell_type":"markdown","metadata":{"id":"J6rZB4d6MfJe"},"source":["# Part 2: Transfer Learning from Electicity to ETTH2"]},{"cell_type":"markdown","metadata":{"id":"IL-JDq7qMfJe"},"source":["In this section, we will demonstrate the transfer learning capability of the `PatchTSMixer` model.\n","We use the model pretrained on Electricity dataset to do zeroshot testing on ETTH2 dataset.\n","\n","\n","In Transfer Learning,  we will pretrain the model for a forecasting task on a `source` dataset. Then, we will use the\n"," pretrained model for zero-shot forecasting on a `target` dataset. The zero-shot forecasting\n"," performance will denote the `test` performance of the model in the `target` domain, without any\n"," training on the target domain. Subsequently, we will do linear probing and (then) finetuning of\n"," the pretrained model on the `train` part of the target data, and will validate the forecasting\n"," performance on the `test` part of the target data. In this example, the source dataset is the Electricity dataset and the target dataset is ETTH2"]},{"cell_type":"markdown","metadata":{"id":"wCyqlZJoMfJe"},"source":["## Transfer Learing on `ETTh2` data. All evaluations are on the `test` part of the `ETTh2` data.\n","Step 1: Directly evaluate the electricity-pretrained model. This is the zero-shot performance.  \n","Step 2: Evalute after doing linear probing.  \n","Step 3: Evaluate after doing full finetuning.  "]},{"cell_type":"markdown","metadata":{"id":"b5pNiys8MfJe"},"source":["### Load ETTh2 data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7H2K8Qj5MfJe"},"outputs":[],"source":["dataset = \"ETTh2\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XSHrekmJMfJe","outputId":"e9b02abe-d2fb-4795-b6a3-502ab822073b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading target dataset: ETTh2\n"]}],"source":["print(f\"Loading target dataset: {dataset}\")\n","dataset_path = f\"https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/{dataset}.csv\"\n","timestamp_column = \"date\"\n","id_columns = []\n","forecast_columns = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n","train_start_index = None  # None indicates beginning of dataset\n","train_end_index = 12 * 30 * 24\n","\n","# we shift the start of the evaluation period back by context length so that\n","# the first evaluation timestamp is immediately following the training data\n","valid_start_index = 12 * 30 * 24 - context_length\n","valid_end_index = 12 * 30 * 24 + 4 * 30 * 24\n","\n","test_start_index = 12 * 30 * 24 + 4 * 30 * 24 - context_length\n","test_end_index = 12 * 30 * 24 + 8 * 30 * 24"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UWRJs72WMfJe","outputId":"198ab2d3-29d2-4bb1-ac5f-af0de5d8312e"},"outputs":[{"data":{"text/plain":["TimeSeriesPreprocessor {\n","  \"context_length\": 64,\n","  \"feature_extractor_type\": \"TimeSeriesPreprocessor\",\n","  \"id_columns\": [],\n","  \"input_columns\": [\n","    \"HUFL\",\n","    \"HULL\",\n","    \"MUFL\",\n","    \"MULL\",\n","    \"LUFL\",\n","    \"LULL\",\n","    \"OT\"\n","  ],\n","  \"output_columns\": [\n","    \"HUFL\",\n","    \"HULL\",\n","    \"MUFL\",\n","    \"MULL\",\n","    \"LUFL\",\n","    \"LULL\",\n","    \"OT\"\n","  ],\n","  \"prediction_length\": null,\n","  \"processor_class\": \"TimeSeriesPreprocessor\",\n","  \"scaler_dict\": {\n","    \"0\": {\n","      \"copy\": true,\n","      \"feature_names_in_\": [\n","        \"HUFL\",\n","        \"HULL\",\n","        \"MUFL\",\n","        \"MULL\",\n","        \"LUFL\",\n","        \"LULL\",\n","        \"OT\"\n","      ],\n","      \"mean_\": [\n","        41.53683496078959,\n","        12.273452896210882,\n","        46.60977329964991,\n","        10.526153112865156,\n","        1.1869920139097505,\n","        -2.373217913729173,\n","        26.872023494265697\n","      ],\n","      \"n_features_in_\": 7,\n","      \"n_samples_seen_\": 8640,\n","      \"scale_\": [\n","        10.448841072588488,\n","        4.587112566531959,\n","        16.858190332598408,\n","        3.018605566682919,\n","        4.641011217319063,\n","        8.460910779279644,\n","        11.584718923414682\n","      ],\n","      \"var_\": [\n","        109.17827976021215,\n","        21.04160169803542,\n","        284.19858129011436,\n","        9.111979567209104,\n","        21.538985119281367,\n","        71.58701121493046,\n","        134.20571253452223\n","      ],\n","      \"with_mean\": true,\n","      \"with_std\": true\n","    }\n","  },\n","  \"scaling\": true,\n","  \"time_series_task\": \"forecasting\",\n","  \"timestamp_column\": \"date\"\n","}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv(\n","    dataset_path,\n","    parse_dates=[timestamp_column],\n",")\n","\n","train_data = select_by_index(\n","    data,\n","    id_columns=id_columns,\n","    start_index=train_start_index,\n","    end_index=train_end_index,\n",")\n","valid_data = select_by_index(\n","    data,\n","    id_columns=id_columns,\n","    start_index=valid_start_index,\n","    end_index=valid_end_index,\n",")\n","test_data = select_by_index(\n","    data,\n","    id_columns=id_columns,\n","    start_index=test_start_index,\n","    end_index=test_end_index,\n",")\n","\n","tsp = TimeSeriesPreprocessor(\n","    timestamp_column=timestamp_column,\n","    id_columns=id_columns,\n","    input_columns=forecast_columns,\n","    output_columns=forecast_columns,\n","    scaling=True,\n",")\n","tsp.train(train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"igHojbDGMfJe"},"outputs":[],"source":["train_dataset = ForecastDFDataset(\n","    tsp.preprocess(train_data),\n","    id_columns=id_columns,\n","    input_columns=forecast_columns,\n","    output_columns=forecast_columns,\n","    context_length=context_length,\n","    prediction_length=forecast_horizon,\n",")\n","valid_dataset = ForecastDFDataset(\n","    tsp.preprocess(valid_data),\n","    id_columns=id_columns,\n","    input_columns=forecast_columns,\n","    output_columns=forecast_columns,\n","    context_length=context_length,\n","    prediction_length=forecast_horizon,\n",")\n","test_dataset = ForecastDFDataset(\n","    tsp.preprocess(test_data),\n","    id_columns=id_columns,\n","    input_columns=forecast_columns,\n","    output_columns=forecast_columns,\n","    context_length=context_length,\n","    prediction_length=forecast_horizon,\n",")"]},{"cell_type":"markdown","metadata":{"id":"4j4g4TaMMfJe"},"source":["## Zero-shot forecasting on `ETTh2`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0G6rOKMyMfJe","outputId":"d531ffb8-fd58-408a-a6bc-ac08e97e0961"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading pretrained model\n","Done\n"]}],"source":["print(\"Loading pretrained model\")\n","finetune_forecast_model = PatchTSMixerForPrediction.from_pretrained(\"patchtsmixer/electricity/model/pretrain/\")\n","print(\"Done\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-q5W4yEMfJe","outputId":"ead16e79-c407-4cf6-8773-ceb90718f115"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Doing zero-shot forecasting on target data\n"]},{"name":"stderr","output_type":"stream","text":["/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='22' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11/11 02:52]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Target data zero-shot forecasting result:\n","{'eval_loss': 0.3038313388824463, 'eval_runtime': 1.8364, 'eval_samples_per_second': 1516.562, 'eval_steps_per_second': 5.99}\n"]}],"source":["finetune_forecast_args = TrainingArguments(\n","    output_dir=\"./checkpoint/patchtsmixer/transfer/finetune/output/\",\n","    overwrite_output_dir=True,\n","    learning_rate=0.0001,\n","    num_train_epochs=100,\n","    do_eval=True,\n","    evaluation_strategy=\"epoch\",\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    dataloader_num_workers=num_workers,\n","    report_to=\"tensorboard\",\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"epoch\",\n","    save_total_limit=3,\n","    logging_dir=\"./checkpoint/patchtsmixer/transfer/finetune/logs/\",  # Make sure to specify a logging directory\n","    load_best_model_at_end=True,  # Load the best model when training ends\n","    metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n","    greater_is_better=False,  # For loss\n",")\n","\n","# Create a new early stopping callback with faster convergence properties\n","early_stopping_callback = EarlyStoppingCallback(\n","    early_stopping_patience=5,  # Number of epochs with no improvement after which to stop\n","    early_stopping_threshold=0.001,  # Minimum improvement required to consider as improvement\n",")\n","\n","finetune_forecast_trainer = Trainer(\n","    model=finetune_forecast_model,\n","    args=finetune_forecast_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=valid_dataset,\n","    callbacks=[early_stopping_callback],\n",")\n","\n","print(\"\\n\\nDoing zero-shot forecasting on target data\")\n","result = finetune_forecast_trainer.evaluate(test_dataset)\n","print(\"Target data zero-shot forecasting result:\")\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"olcm8002MfJf"},"source":["By a direct zeroshot, we get MSE of 0.3 which is near to the SOTA result. Lets see, how we can do a simple linear probing to match the SOTA results."]},{"cell_type":"markdown","metadata":{"id":"c0ByeTk2MfJf"},"source":["## Target data `ETTh2` linear probing\n","We can do a quick linear probing on the `train` part of the target data to see any possible `test` performance improvement."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HQrDWpC-MfJf","outputId":"d4910d0d-6a10-43b6-bd67-0f5060d82e97"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Linear probing on the target data\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='416' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 416/3200 01:01 < 06:53, 6.73 it/s, Epoch 13/100]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.447000</td>\n","      <td>0.216436</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.438600</td>\n","      <td>0.215667</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.429400</td>\n","      <td>0.215104</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.422500</td>\n","      <td>0.213820</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.418500</td>\n","      <td>0.213585</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.415000</td>\n","      <td>0.213016</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.412000</td>\n","      <td>0.213067</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.412400</td>\n","      <td>0.211993</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.405900</td>\n","      <td>0.212460</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.405300</td>\n","      <td>0.211772</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.406200</td>\n","      <td>0.212154</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.400600</td>\n","      <td>0.212082</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.405300</td>\n","      <td>0.211458</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating\n"]},{"name":"stderr","output_type":"stream","text":["/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11/11 00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Target data head/linear probing result:\n","{'eval_loss': 0.27119266986846924, 'eval_runtime': 1.7621, 'eval_samples_per_second': 1580.478, 'eval_steps_per_second': 6.242, 'epoch': 13.0}\n"]}],"source":["# Freeze the backbone of the model\n","for param in finetune_forecast_trainer.model.model.parameters():\n","    param.requires_grad = False\n","\n","print(\"\\n\\nLinear probing on the target data\")\n","finetune_forecast_trainer.train()\n","print(\"Evaluating\")\n","result = finetune_forecast_trainer.evaluate(test_dataset)\n","print(\"Target data head/linear probing result:\")\n","print(result)"]},{"cell_type":"markdown","metadata":{"vscode":{"languageId":"plaintext"},"id":"Dcq0J-plMfJf"},"source":["By doing a simple linear probing, MSE decreased from 0.3 to 0.271 achiving the SOTA results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0eqR3ZusMfJf","outputId":"e16fe0e3-d5aa-4934-b1d5-c9319734c3b8"},"outputs":[{"data":{"text/plain":["['patchtsmixer/electricity/model/transfer/ETTh2/preprocessor/preprocessor_config.json']"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["save_dir = f\"patchtsmixer/electricity/model/transfer/{dataset}/model/linear_probe/\"\n","os.makedirs(save_dir, exist_ok=True)\n","finetune_forecast_trainer.save_model(save_dir)\n","\n","save_dir = f\"patchtsmixer/electricity/model/transfer/{dataset}/preprocessor/\"\n","os.makedirs(save_dir, exist_ok=True)\n","tsp.save_pretrained(save_dir)"]},{"cell_type":"markdown","metadata":{"id":"jq56VqQQMfJf"},"source":["Lets now see, if we get any more improvements by doing a full finetune."]},{"cell_type":"markdown","metadata":{"id":"uDyRIP3OMfJf"},"source":["## Target data `ETTh2` full finetune\n","\n","We can do a full model finetune (instead of probing the last linear layer as shown above) on the `train` part of the target data to see a possible `test` performance improvement."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZfUQhDqGMfJf","outputId":"bb3d96b4-5d4a-4053-ade5-449fa631e730"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Finetuning on the target data\n"]},{"name":"stderr","output_type":"stream","text":["/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='288' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 288/3200 00:44 < 07:34, 6.40 it/s, Epoch 9/100]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.432900</td>\n","      <td>0.215200</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.416700</td>\n","      <td>0.210919</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.401400</td>\n","      <td>0.209932</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.392900</td>\n","      <td>0.208808</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.388100</td>\n","      <td>0.209692</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.375900</td>\n","      <td>0.209546</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.370000</td>\n","      <td>0.210207</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.367000</td>\n","      <td>0.211601</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.359400</td>\n","      <td>0.211405</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating\n"]},{"name":"stderr","output_type":"stream","text":["/dccstor/dnn_forecasting/conda_envs/envs/hf/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11/11 00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Target data full finetune result:\n","{'eval_loss': 0.2734043300151825, 'eval_runtime': 1.5853, 'eval_samples_per_second': 1756.725, 'eval_steps_per_second': 6.939, 'epoch': 9.0}\n"]}],"source":["# Reload the model\n","finetune_forecast_model = PatchTSMixerForPrediction.from_pretrained(\"patchtsmixer/electricity/model/pretrain/\")\n","finetune_forecast_trainer = Trainer(\n","    model=finetune_forecast_model,\n","    args=finetune_forecast_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=valid_dataset,\n","    callbacks=[early_stopping_callback],\n",")\n","print(\"\\n\\nFinetuning on the target data\")\n","finetune_forecast_trainer.train()\n","print(\"Evaluating\")\n","result = finetune_forecast_trainer.evaluate(test_dataset)\n","print(\"Target data full finetune result:\")\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"YjPGY8gjMfJl"},"source":["There is not much improvement with ETTH2 dataset with full finetuning. Lets save the model anyway."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5HvCSlOMfJl"},"outputs":[],"source":["save_dir = f\"patchtsmixer/electricity/model/transfer/{dataset}/model/fine_tuning/\"\n","os.makedirs(save_dir, exist_ok=True)\n","finetune_forecast_trainer.save_model(save_dir)"]},{"cell_type":"markdown","metadata":{"id":"hwEHP3D3MfJl"},"source":["\n","Summary: In this blog, we presented a step-by-step guide on leveraging PatchTSMixer for tasks related to forecasting and transfer learning. We intend to facilitate the seamless integration of the PatchTSMixer HF model for your forecasting use cases. We trust that this content serves as a useful resource to expedite your adoption of PatchTSMixer. Thank you for tuning in to our blog, and we hope you find this information beneficial for your projects.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tm1WSxTjMfJl"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}